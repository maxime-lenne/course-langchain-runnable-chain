{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84f4459b",
   "metadata": {},
   "source": [
    "![LangChain](img/langchain.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2511a0f0",
   "metadata": {},
   "source": [
    "**LangChain** permet d‚Äôencha√Æner facilement diff√©rents composants de traitement dans un **pipeline unifi√©**. Ces composants ‚Äî qu‚Äôil s‚Äôagisse d‚Äôun **prompt**, d‚Äôun **mod√®le de langage** ou d‚Äôun **outil externe** ‚Äî sont tous trait√©s comme des `Runnable`, c‚Äôest-√†-dire des **blocs interop√©rables pouvant √™tre connect√©s les uns aux autres**.\n",
    "\n",
    "Gr√¢ce √† cette architecture, il devient simple de construire des cha√Ænes logiques de traitement par exemple :\n",
    "\n",
    "> **g√©n√©rer un prompt** ‚Üí **l‚Äôenvoyer √† un LLM** ‚Üí **interpr√©ter la r√©ponse** ‚Üí **puis appeler une API ou une fonction locale**\n",
    "\n",
    "C'est avec le ***LangChain Expression Language*** (LCEL) que nous pouvons cha√Æner les composants via l‚Äôop√©rateur `|` (le pipe) et d‚Äôex√©cuter le tout de mani√®re uniforme avec `.invoke()`.\n",
    "\n",
    "Gr√¢ce aux `chains`, nous pouvons r√©sumer **Langchain** √† ceci :\n",
    "\n",
    "> Bo√Æte √† outils pour cr√©er des pipelines modulaires, r√©utilisables et tra√ßables autour des mod√®les de langage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfaee202",
   "metadata": {},
   "source": [
    "![Chains](img/chains.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7eb01a",
   "metadata": {},
   "source": [
    "# 1. Chargement du mod√®le LLM local\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cafb689",
   "metadata": {},
   "source": [
    "Dans cette section, nous chargeons un mod√®le de langage local gr√¢ce √† **Ollama**. Cela permet de travailler avec un **LLM directement sur notre machine**, sans connexion √† une API externe.\n",
    "\n",
    "Nous utilisons ici la classe `ChatOllama` de **LangChain**, qui nous permet d‚Äôinteragir facilement avec un mod√®le comme llama3 d√©j√† t√©l√©charg√© via Ollama."
   ]
  },
  {
   "cell_type": "code",
   "id": "7301c899",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T10:45:40.183167Z",
     "start_time": "2026-02-09T10:45:39.563457Z"
    }
   },
   "source": [
    "import os\n",
    "from IPython.display import display, Markdown\n",
    "from dotenv import load_dotenv\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_deepseek import ChatDeepSeek\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnableBranch\n",
    "\n",
    "\n",
    "# Chargement des cl√©s d'API se trouvant dans le fichier .env.\n",
    "# Ceci permet d'utiliser des mod√®les en ligne comme gpt-x, deepseek-x, etc...\n",
    "load_dotenv(override=True)\n",
    "\n",
    "model = ChatOllama(model=\"llama3\")\n",
    "#model = ChatDeepSeek(model=\"deepseek-chat\", api_key=os.getenv(\"DEEPSEEK_API_KEY\"))"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain.prompts'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mModuleNotFoundError\u001B[39m                       Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 8\u001B[39m\n\u001B[32m      6\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlangchain_core\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01moutput_parsers\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m StrOutputParser\n\u001B[32m      7\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlangchain_core\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mrunnables\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m RunnableLambda\n\u001B[32m----> \u001B[39m\u001B[32m8\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlangchain\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mprompts\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m ChatPromptTemplate\n\u001B[32m      9\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlangchain_core\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mrunnables\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m RunnableParallel, RunnableBranch\n\u001B[32m     12\u001B[39m \u001B[38;5;66;03m# Chargement des cl√©s d'API se trouvant dans le fichier .env.\u001B[39;00m\n\u001B[32m     13\u001B[39m \u001B[38;5;66;03m# Ceci permet d'utiliser des mod√®les en ligne comme gpt-x, deepseek-x, etc...\u001B[39;00m\n",
      "\u001B[31mModuleNotFoundError\u001B[39m: No module named 'langchain.prompts'"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "a48a0cd4",
   "metadata": {},
   "source": [
    "# 2. Cha√Æne basique\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdc2daf",
   "metadata": {},
   "source": [
    "Une cha√Æne de traitement simple peut √™tre construite en combinant un prompt structur√© avec un mod√®le de langage √† l‚Äôaide du syst√®me de cha√Ænage de LangChain.\n",
    "Ce type de cha√Æne permet de cr√©er un dialogue en d√©finissant plusieurs r√¥les (comme system et human) et en injectant dynamiquement des valeurs dans le prompt."
   ]
  },
  {
   "cell_type": "code",
   "id": "da43aef4",
   "metadata": {},
   "source": [
    "# On d√©finit une liste de messages structur√©s pour guider le comportement du mod√®le.\n",
    "# ‚ö†Ô∏è Ici, on utilise des TUPLES (r√¥le, message avec variables), c‚Äôest n√©cessaire pour que l‚Äôinterpolation des variables fonctionne avec from_messages().\n",
    "# ‚ö†Ô∏è L'interpolation avec des objets comme `HumanMessage(content=\"...\")` ou `SystemMessage(content=\"...\")` ne fonctionne PAS directement avec from_messages().\n",
    "# Ces objets sont con√ßus pour des messages d√©j√† complets, pas des templates avec des variables.\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Tu es un expert en math√©matiques et un p√©dagogue dans ce domaine.\"),\n",
    "    (\"human\", \"Calcule le double de {value_1}, puis celui de {value_2}\")\n",
    "])\n",
    "\n",
    "# √âquivalent d'un template √† r√¥le unique\n",
    "# template = \"Tu es un expert en math√©matiques et un p√©dagogue dans ce domaine. Calcule le double de {value_1}, puis celui de {value_2}.\"\n",
    "# prompt_template = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# On relie le prompt au mod√®le √† l‚Äôaide de l‚Äôop√©rateur |\n",
    "chain = prompt_template | model\n",
    "\n",
    "# On fournit des valeurs aux variables d√©finies dans le prompt\n",
    "result = chain.invoke({\"value_1\": 4, \"value_2\": 2})\n",
    "\n",
    "display(Markdown(result.content))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b70e062a",
   "metadata": {},
   "source": [
    "### üß© Exercices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543a840f",
   "metadata": {},
   "source": [
    "> Exercice 1\n",
    "\n",
    "Cr√©ez un prompt qui demande √† un mod√®le de d√©finir un mot donn√©, dans un style p√©dagogique.\n",
    "\n",
    "1.\tUtilisez ChatPromptTemplate.from_messages() pour d√©finir un prompt structur√© avec :\n",
    "- un message system : l‚ÄôIA est un professeur d'un domaine particulier qui explique simplement.\n",
    "- un message human : l‚Äôutilisateur demande la d√©finition d‚Äôun mot particulier.\n",
    "2.\tRelie ce prompt √† un mod√®le avec l‚Äôop√©rateur |.\n",
    "3.\tUtilise .invoke() pour tester le prompt avec plusieurs disciplines et th√®mes diff√©rents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d3e0b3",
   "metadata": {},
   "source": [
    "# 3. Cha√Æne √©tendue (s√©quence de runnables)\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdab574c",
   "metadata": {},
   "source": [
    "L‚Äôun des atouts majeurs de LangChain r√©side dans son syst√®me de **cha√Ænes composables**, o√π chaque composant du pipeline est un `Runnable`. Gr√¢ce √† l‚Äôop√©rateur `|`, on peut encha√Æner autant d'√©tapes de traitement que voulu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6049d905",
   "metadata": {},
   "source": [
    "### 3.1 Runnable built-in"
   ]
  },
  {
   "cell_type": "code",
   "id": "81c81a83",
   "metadata": {},
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Tu es un expert en math√©matiques et un p√©dagogue dans ce domaine.\"),\n",
    "    (\"human\", \"Calcule le double de {value_1}, puis celui de {value_2}\")\n",
    "])\n",
    "\n",
    "# Ce parseur prend la sortie brute du mod√®le (souvent du texte) et la convertit en cha√Æne de caract√®res simple pour faciliter la suite.\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# Encha√Ænement de runnables\n",
    "chain = prompt_template | model | parser\n",
    "\n",
    "result = chain.invoke({\"value_1\": 4, \"value_2\": 2})\n",
    "\n",
    "# Affichage du r√©sultat retourn√© par le mod√®le apr√®s parsing. Plus besoin du `.content`\n",
    "display(Markdown(result))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0f877d27",
   "metadata": {},
   "source": [
    "### 3.2 Runnable custom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03672db2",
   "metadata": {},
   "source": [
    "Langchain offre non seulement d‚Äôutiliser des composants pr√©d√©finis (LLMs, parsers, prompts‚Ä¶) comme √©voqu√© pr√©c√©demment, mais aussi de d√©finir facilement ses propres blocs de traitement.\n",
    "\n",
    "Gr√¢ce √† la classe `RunnableLambda`, on peut transformer n‚Äôimporte quelle fonction Python en un maillon de la cha√Æne. Cela ouvre la porte √† un nombre infini de transformations : nettoyage de texte, post-traitement, extraction de donn√©es, formatage, journalisation, etc."
   ]
  },
  {
   "cell_type": "code",
   "id": "8e85ab31",
   "metadata": {},
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Tu es un expert en math√©matiques et un p√©dagogue dans ce domaine.\"),\n",
    "    (\"human\", \"Calcule le double de {value_1}, puis celui de {value_2}\")\n",
    "])\n",
    "\n",
    "parser = StrOutputParser()\n",
    "uppercase = RunnableLambda(lambda x: x.upper()) # Runnable custom pour transformer la sortie en majuscules\n",
    "\n",
    "# Encha√Ænement de runnables\n",
    "chain = prompt_template | model | parser | uppercase\n",
    "\n",
    "result = chain.invoke({\"value_1\": 4, \"value_2\": 2})\n",
    "\n",
    "display(Markdown(result))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e333bf7b",
   "metadata": {},
   "source": [
    "### üß© Exercices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57787ca8",
   "metadata": {},
   "source": [
    "> Exercice 1\n",
    "\n",
    "Cr√©ez un pipeline qui r√©pond √† des questions clients ou formule des messages marketing. Il faut que ces r√©ponses soient :\n",
    "- stylis√©es,\n",
    "- enrichies,\n",
    "- adapt√©es √† diff√©rents formats de publication."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f21b621",
   "metadata": {},
   "source": [
    "# 4. Cha√Ænes parall√®les\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497574e0",
   "metadata": {},
   "source": [
    "### 4.1 Cha√Ænes parall√®les avec post-traitement externe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce763ed",
   "metadata": {},
   "source": [
    "Dans LangChain, il est possible d‚Äôex√©cuter plusieurs **cha√Ænes de traitement en parall√®le** √† l‚Äôaide du composant `RunnableParallel`. Cela permet, par exemple, d‚Äôeffectuer plusieurs op√©rations ind√©pendantes"
   ]
  },
  {
   "cell_type": "code",
   "id": "e798a6fd",
   "metadata": {},
   "source": [
    "system_role = (\"system\", \"Tu es un expert en math√©matiques.\")\n",
    "\n",
    "# Prompt pour additionner\n",
    "prompt_add = ChatPromptTemplate.from_messages([\n",
    "    system_role,\n",
    "    (\"human\", \"Additionne {value_1} √† {value_2}.\")\n",
    "])\n",
    "\n",
    "# Prompt pour soustraire\n",
    "prompt_substract = ChatPromptTemplate.from_messages([\n",
    "    system_role,\n",
    "    (\"human\", \"Soustrais {value_1} de {value_2}.\")\n",
    "])\n",
    "\n",
    "# Cha√Ænes s√©par√©es\n",
    "chain_add = prompt_add | model | StrOutputParser()\n",
    "chain_substract = prompt_substract | model | StrOutputParser()\n",
    "\n",
    "# Traitement parall√®le √† ex√©cuter\n",
    "parallel_chain = RunnableParallel({\n",
    "    \"add\": chain_add,\n",
    "    \"substract\": chain_substract\n",
    "})\n",
    "\n",
    "# M√™me jeu de donn√©es utilis√© pour les deux cha√Ænes\n",
    "inputs = {\"value_1\": 10, \"value_2\": 4}\n",
    "\n",
    "# Ex√©cution des traitements en parall√®le\n",
    "result = parallel_chain.invoke(inputs)\n",
    "\n",
    "# Affichage\n",
    "print(\"R√©sultat de l'addition :\\n\")\n",
    "display(Markdown(result[\"add\"]))\n",
    "print(\"\\nR√©sultat de la soustraction :\\n\")\n",
    "display(Markdown(result[\"substract\"]))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d8abc614",
   "metadata": {},
   "source": [
    "### 4.1 Cha√Ænes parall√®les avec post-traitement int√©gr√© dans la cha√Æne"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1247d0",
   "metadata": {},
   "source": [
    "Pour √©viter de manipuler manuellement les r√©sultats (comme result[\"add\"] ou result[\"substract\"]), il est possible d‚Äôajouter un bloc de post-traitement directement √† la fin de la cha√Æne parall√®le gr√¢ce √† RunnableLambda.\n",
    "\n",
    "Cette approche permet de :\n",
    "- structurer la sortie de mani√®re centralis√©e,\n",
    "- int√©grer la logique m√©tier ou d‚Äôaffichage directement dans le pipeline.\n",
    "\n",
    "C‚Äôest une bonne pratique lorsqu‚Äôon souhaite rendre une cha√Æne modulaire, maintenable et r√©utilisable dans un syst√®me plus large (ex. : API, application, chatbot‚Ä¶)."
   ]
  },
  {
   "cell_type": "code",
   "id": "727f677b",
   "metadata": {},
   "source": [
    "system_role = (\"system\", \"Tu es un expert en math√©matiques.\")\n",
    "\n",
    "# Prompts\n",
    "prompt_add = ChatPromptTemplate.from_messages([\n",
    "    system_role,\n",
    "    (\"human\", \"Additionne {value_1} √† {value_2}.\")\n",
    "])\n",
    "\n",
    "prompt_substract = ChatPromptTemplate.from_messages([\n",
    "    system_role,\n",
    "    (\"human\", \"Soustrais {value_1} de {value_2}.\")\n",
    "])\n",
    "\n",
    "# Cha√Ænes\n",
    "chain_add = prompt_add | model | StrOutputParser()\n",
    "chain_substract = prompt_substract | model | StrOutputParser()\n",
    "\n",
    "# Traitement parall√®le\n",
    "parallel_chain = RunnableParallel({\n",
    "    \"addition\": chain_add,\n",
    "    \"soustraction\": chain_substract\n",
    "})\n",
    "\n",
    "# Post-traitement avec RunnableLambda\n",
    "postprocess = RunnableLambda(lambda result:\n",
    "f\"\"\"R√©sultats du traitement parall√®le :\n",
    "- Addition : {result[\"addition\"].strip()}\n",
    "- Soustraction : {result[\"soustraction\"].strip()}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Cha√Æne finale\n",
    "full_chain = parallel_chain | postprocess\n",
    "\n",
    "# Entr√©e\n",
    "inputs = {\"value_1\": 10, \"value_2\": 4}\n",
    "\n",
    "# R√©sultat\n",
    "result = full_chain.invoke(inputs)\n",
    "\n",
    "display(Markdown(result))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "44ada11a",
   "metadata": {},
   "source": [
    "### üß© Exercices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1e5ddf",
   "metadata": {},
   "source": [
    "> Exercice 1\n",
    "\n",
    "Construire une mini-analyseur de texte. √Ä partir d‚Äôun m√™me paragraphe, nous voulons :\n",
    "- R√©sumer le texte\n",
    "- Extraire les mots-cl√©s\n",
    "- D√©tecter la langue\n",
    "- Analyser le sentiment\n",
    "\n",
    "Vous pouvez suivre ce sch√©ma :\n",
    "1. Cr√©er les prompts\n",
    "2. Cr√©er les cha√Ænes\n",
    "3. Assembler les cha√Ænes\n",
    "4. Pr√©parer les inputs\n",
    "5. Lancer le traitement et afficher les r√©sultats"
   ]
  },
  {
   "cell_type": "code",
   "id": "cd7db21d",
   "metadata": {},
   "source": [
    "# Votre code ici"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c2ca6e28",
   "metadata": {},
   "source": [
    "# 5. Branches conditionnelles\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d670faf9",
   "metadata": {},
   "source": [
    "Il est possible de d√©finir des chemins conditionnels dans un pipeline, on parle alors de branche conditionnelle.\n",
    "\n",
    "Gr√¢ce √† `RunnableBranch`, il est possible de router dynamiquement la sortie d‚Äôun composant (comme un LLM) vers diff√©rents traitements en fonction de son contenu ou de n‚Äôimporte quelle r√®gle m√©tier.\n",
    "\n",
    "Dans l'exemple qui suit :\n",
    "\n",
    "1. On demande au LLM de calculer le double d‚Äôune valeur et de retourner uniquement un r√©sultat num√©rique brut.\n",
    "2. On analyse ce r√©sultat :\n",
    "- Si le r√©sultat est sup√©rieur ou √©gal √† 100, on le met en majuscules et on affiche un message adapt√©.\n",
    "- Sinon, on l‚Äôaffiche en minuscules avec un message diff√©rent.\n",
    "3. Tout cela est encapsul√© dans une cha√Æne principale.\n",
    "\n",
    "Ce m√©canisme est extr√™mement utile pour adapter dynamiquement le comportement d‚Äôune IA √† diff√©rents contextes : affichage, r√®gles m√©tier, logique m√©tier avanc√©e ou traitements sp√©cialis√©s."
   ]
  },
  {
   "cell_type": "code",
   "id": "99fa780c",
   "metadata": {},
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Tu es un expert en math√©matiques et un p√©dagogue dans ce domaine.\"),\n",
    "    (\"human\", \"Calcule le double de {value}. Retourne uniquement le r√©sulat sous forme de nombre, sans explications ou autres types de texte.\")\n",
    "])\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "base_chain = prompt_template | model | parser\n",
    "\n",
    "# Runnables de traitement et de formatage\n",
    "uppercase = RunnableLambda(lambda x: f\"Le r√©sultat est {x} (>= 100), transformation en majuscules.\".upper())\n",
    "lowercase = RunnableLambda(lambda x: f\"Le r√©sultat est {x} (< 100), tout en minuscules.\".lower())\n",
    "\n",
    "# Branche selon le contenu g√©n√©r√©\n",
    "branch = RunnableBranch(\n",
    "    (lambda x: int(x) >= 100, uppercase),\n",
    "    lowercase\n",
    ")\n",
    "\n",
    "# Cha√Æne compl√®te : on applique d‚Äôabord le LLM, puis on branche\n",
    "chain = base_chain | branch\n",
    "\n",
    "result = chain.invoke({\"value\": 1})\n",
    "display(Markdown(result))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7ef1cf1d",
   "metadata": {},
   "source": [
    "### üß© Exercices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36febf86",
   "metadata": {},
   "source": [
    "> Exercice 1\n",
    "\n",
    "Sur une fiche produit e-commerce, les clients laissent des commentaires vari√©s. L‚Äôobjectif est de construire une cha√Æne intelligente capable de r√©pondre √† chacun de ces commentaires de mani√®re empathique et appropri√©e, sans intervention humaine.\n",
    "\n",
    "Construire une cha√Æne LangChain **enti√®rement automatis√©e**, dans laquelle un mod√®le de langage (LLM) :\n",
    "\n",
    "1.\tAnalyse un commentaire client brut,\n",
    "2.\tD√©tecte la tonalit√© du message (positive, negative, neutral),\n",
    "3.\tEt g√©n√®re une r√©ponse adapt√©e, en s√©lectionnant dynamiquement le bon ton de r√©ponse via un branchement conditionnel (RunnableBranch).\n",
    "\n",
    "**Exemple :**\n",
    "\n",
    "\"J‚Äôai bien re√ßu le produit, mais l‚Äôemballage √©tait ab√Æm√©.\"\n",
    "\n",
    "‚û°Ô∏è Le LLM doit d√©tecter un sentiment n√©gatif, puis router vers une r√©ponse du type :\n",
    "\n",
    "\"Nous sommes d√©sol√©s d‚Äôapprendre cela. Pourriez-vous nous donner plus de d√©tails ou contacter notre support afin que nous puissions r√©soudre le probl√®me ?\"\n",
    "\n",
    "\n",
    "\n",
    "üí° **Pour vous aider, vous pouvez suivre ces √©tapes :**\n",
    "\n",
    "1.  Cr√©ation d‚Äôune premi√®re cha√Æne : un prompt demande au LLM d‚Äôanalyser un commentaire client et de retourner uniquement le sentiment (positive, negative, neutral).\n",
    "2. Cr√©ation de trois fonctions (ou RunnableLambda) :\n",
    "- Pour r√©pondre positivement : remercier et encourager.\n",
    "- Pour r√©pondre √† un avis n√©gatif : exprimer des regrets, demander plus de d√©tails ou proposer de contacter le support.\n",
    "- Pour un avis neutre : offrir son aide et demander si le client souhaite en savoir plus.\n",
    "3. Utilisation de RunnableBranch pour appliquer le bon traitement selon le sentiment d√©tect√©.\n",
    "4. Regrouper le tout dans une cha√Æne compl√®te :\n",
    "- Entr√©e : un commentaire client (texte brut)\n",
    "- Sortie : une r√©ponse adapt√©e au ton d√©tect√©."
   ]
  },
  {
   "cell_type": "code",
   "id": "11c88080",
   "metadata": {},
   "source": [
    "# Votre code ici"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
