# Runnable Chains - LangChain

![LangChain](img/langchain.jpeg)

**LangChain** permet d'encha√Æner facilement diff√©rents composants de traitement dans un
**pipeline unifi√©**. Ces composants ‚Äî qu'il s'agisse d'un **prompt**, d'un **mod√®le de langage**
ou d'un **outil externe** ‚Äî sont tous trait√©s comme des `Runnable`, c'est-√†-dire des
**blocs interop√©rables pouvant √™tre connect√©s les uns aux autres**.

Gr√¢ce √† cette architecture, il devient simple de construire des cha√Ænes logiques de traitement
par exemple :

> **g√©n√©rer un prompt** ‚Üí **l'envoyer √† un LLM** ‚Üí **interpr√©ter la r√©ponse** ‚Üí
> **puis appeler une API ou une fonction locale**

C'est avec le ***LangChain Expression Language*** (LCEL) que nous pouvons cha√Æner les composants
via l'op√©rateur `|` (le pipe) et d'ex√©cuter le tout de mani√®re uniforme avec `.invoke()`.

Gr√¢ce aux `chains`, nous pouvons r√©sumer **Langchain** √† ceci :

> Bo√Æte √† outils pour cr√©er des pipelines modulaires, r√©utilisables et tra√ßables autour des
> mod√®les de langage.

![Chains](img/chains.png)

## 1. Chargement du mod√®le LLM local

Dans cette section, nous chargeons un mod√®le de langage local gr√¢ce √† **Ollama**. Cela permet
de travailler avec un **LLM directement sur notre machine**, sans connexion √† une API externe.

Nous utilisons ici la classe `ChatOllama` de **LangChain**, qui nous permet d'interagir
facilement avec un mod√®le comme llama3 d√©j√† t√©l√©charg√© via Ollama.

## 2. Cha√Æne basique

Une cha√Æne de traitement simple peut √™tre construite en combinant un prompt structur√© avec un
mod√®le de langage √† l'aide du syst√®me de cha√Ænage de LangChain.
Ce type de cha√Æne permet de cr√©er un dialogue en d√©finissant plusieurs r√¥les (comme system et
human) et en injectant dynamiquement des valeurs dans le prompt.

### üß© Exercices

> Exercice 1

Cr√©ez un prompt qui demande √† un mod√®le de d√©finir un mot donn√©, dans un style p√©dagogique.

1. Utilisez ChatPromptTemplate.from_messages() pour d√©finir un prompt structur√© avec :

- un message system : l'IA est un professeur d'un domaine particulier qui explique simplement.
- un message human : l'utilisateur demande la d√©finition d'un mot particulier.

1. Relie ce prompt √† un mod√®le avec l'op√©rateur |.
2. Utilise .invoke() pour tester le prompt avec plusieurs disciplines et th√®mes diff√©rents.

## 3. Cha√Æne √©tendue (s√©quence de runnables)

L'un des atouts majeurs de LangChain r√©side dans son syst√®me de **cha√Ænes composables**, o√π
chaque composant du pipeline est un `Runnable`. Gr√¢ce √† l'op√©rateur `|`, on peut encha√Æner
autant d'√©tapes de traitement que voulu.

### 3.1 Runnable built-in

### 3.2 Runnable custom

Langchain offre non seulement d'utiliser des composants pr√©d√©finis (LLMs, parsers, prompts‚Ä¶)
comme √©voqu√© pr√©c√©demment, mais aussi de d√©finir facilement ses propres blocs de traitement.

Gr√¢ce √† la classe `RunnableLambda`, on peut transformer n'importe quelle fonction Python en un
maillon de la cha√Æne. Cela ouvre la porte √† un nombre infini de transformations : nettoyage de
texte, post-traitement, extraction de donn√©es, formatage, journalisation, etc.

### üß© Exercices

> Exercice 1

Cr√©ez un pipeline qui r√©pond √† des questions clients ou formule des messages marketing.
Il faut que ces r√©ponses soient :

- stylis√©es,
- enrichies,
- adapt√©es √† diff√©rents formats de publication.

## 4. Cha√Ænes parall√®les

### 4.1 Cha√Ænes parall√®les avec post-traitement externe

Dans LangChain, il est possible d'ex√©cuter plusieurs **cha√Ænes de traitement en parall√®le** √†
l'aide du composant `RunnableParallel`. Cela permet, par exemple, d'effectuer plusieurs
op√©rations ind√©pendantes

### 4.2 Cha√Ænes parall√®les avec post-traitement int√©gr√© dans la cha√Æne

Pour √©viter de manipuler manuellement les r√©sultats (comme result["add"] ou result["substract"]),
il est possible d'ajouter un bloc de post-traitement directement √† la fin de la cha√Æne parall√®le
gr√¢ce √† RunnableLambda.

Cette approche permet de :

- structurer la sortie de mani√®re centralis√©e,
- int√©grer la logique m√©tier ou d'affichage directement dans le pipeline.

C'est une bonne pratique lorsqu'on souhaite rendre une cha√Æne modulaire, maintenable et
r√©utilisable dans un syst√®me plus large (ex. : API, application, chatbot‚Ä¶).

### üß© Exercices

> Exercice 1

Construire une mini-analyseur de texte. √Ä partir d'un m√™me paragraphe, nous voulons :

- R√©sumer le texte
- Extraire les mots-cl√©s
- D√©tecter la langue
- Analyser le sentiment

Vous pouvez suivre ce sch√©ma :

1. Cr√©er les prompts
2. Cr√©er les cha√Ænes
3. Assembler les cha√Ænes
4. Pr√©parer les inputs
5. Lancer le traitement et afficher les r√©sultats

## 5. Branches conditionnelles

Il est possible de d√©finir des chemins conditionnels dans un pipeline, on parle alors de branche
conditionnelle.

Gr√¢ce √† `RunnableBranch`, il est possible de router dynamiquement la sortie d'un composant
(comme un LLM) vers diff√©rents traitements en fonction de son contenu ou de n'importe quelle
r√®gle m√©tier.

Dans l'exemple qui suit :

1. On demande au LLM de calculer le double d'une valeur et de retourner uniquement un r√©sultat
   num√©rique brut.
2. On analyse ce r√©sultat :

- Si le r√©sultat est sup√©rieur ou √©gal √† 100, on le met en majuscules et on affiche un message
  adapt√©.
- Sinon, on l'affiche en minuscules avec un message diff√©rent.

1. Tout cela est encapsul√© dans une cha√Æne principale.

Ce m√©canisme est extr√™mement utile pour adapter dynamiquement le comportement d'une IA √†
diff√©rents contextes : affichage, r√®gles m√©tier, logique m√©tier avanc√©e ou traitements
sp√©cialis√©s.

### üß© Exercices

> Exercice 1

Sur une fiche produit e-commerce, les clients laissent des commentaires vari√©s. L'objectif est
de construire une cha√Æne intelligente capable de r√©pondre √† chacun de ces commentaires de
mani√®re empathique et appropri√©e, sans intervention humaine.

Construire une cha√Æne LangChain **enti√®rement automatis√©e**, dans laquelle un mod√®le de langage
(LLM) :

1. Analyse un commentaire client brut,
2. D√©tecte la tonalit√© du message (positive, negative, neutral),
3. Et g√©n√®re une r√©ponse adapt√©e, en s√©lectionnant dynamiquement le bon ton de r√©ponse via un
   branchement conditionnel (RunnableBranch).

**Exemple :**

"J'ai bien re√ßu le produit, mais l'emballage √©tait ab√Æm√©."

‚û°Ô∏è Le LLM doit d√©tecter un sentiment n√©gatif, puis router vers une r√©ponse du type :

"Nous sommes d√©sol√©s d'apprendre cela. Pourriez-vous nous donner plus de d√©tails ou contacter
notre support afin que nous puissions r√©soudre le probl√®me ?"

üí° **Pour vous aider, vous pouvez suivre ces √©tapes :**

1. Cr√©ation d'une premi√®re cha√Æne : un prompt demande au LLM d'analyser un commentaire client
   et de retourner uniquement le sentiment (positive, negative, neutral).
2. Cr√©ation de trois fonctions (ou RunnableLambda) :

- Pour r√©pondre positivement : remercier et encourager.
- Pour r√©pondre √† un avis n√©gatif : exprimer des regrets, demander plus de d√©tails ou proposer
  de contacter le support.
- Pour un avis neutre : offrir son aide et demander si le client souhaite en savoir plus.

1. Utilisation de RunnableBranch pour appliquer le bon traitement selon le sentiment d√©tect√©.
2. Regrouper le tout dans une cha√Æne compl√®te :

- Entr√©e : un commentaire client (texte brut)
- Sortie : une r√©ponse adapt√©e au ton d√©tect√©.
